{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f4140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d5285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 18:22:50,405 INFO: Initializing external client\n",
      "2026-01-05 18:22:50,413 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-05 18:22:52,317 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1267871\n",
      "Successfully connected to Feature Store: a1id2223_featurestore\n",
      "HSFS Version: 4.2.10\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "# 1. Login\n",
    "project = hopsworks.login()\n",
    "\n",
    "# 2. Get the Feature Store (This triggers the metadata check)\n",
    "try:\n",
    "    fs = project.get_feature_store(\"A1ID2223\")\n",
    "    print(f\"Successfully connected to Feature Store: {fs.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Feature Store Connection Error: {e}\")\n",
    "\n",
    "# 3. Check versions\n",
    "print(f\"HSFS Version: {hsfs.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0745085",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = fs.get_feature_group(name=\"sentiments\", version=2)\n",
    "opening = fs.get_feature_group(name=\"opening_prices\", version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31136c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.50s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.82s) \n"
     ]
    }
   ],
   "source": [
    "sentiments = sentiments.read()\n",
    "opening = opening.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c2b03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2026-01-05 00:00:00+0000', tz='Etc/UTC')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f592302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2026-01-05 00:00:00+0000', tz='Etc/UTC')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opening['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdd2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentiment and opening price data\n",
    "merged = sentiments.merge(opening, on='date', how='inner')  # Inner join drops weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b640852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 1138 dates with both sentiment and opening prices\n"
     ]
    }
   ],
   "source": [
    "# Filter to only dates where we have both sentiment and opening prices\n",
    "merged = sentiments.merge(opening, on='date', how='inner', suffixes=('_sent', '_open'))\n",
    "print(f\"After filtering: {len(merged)} dates with both sentiment and opening prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b3b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing target_open values using next day's open price.\n",
      "Remaining null target_open: 1 (should be 1 for the last date)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FIX: Fill missing target_open values\n",
    "# ---------------------------------------------------------\n",
    "# The daily ingestion script appends new data with target_open=NaN but doesn't \n",
    "# update the previous day's target. We can fix this here by shifting the open price.\n",
    "\n",
    "# Sort by date to ensure shift works correctly\n",
    "merged = merged.sort_values('date')\n",
    "\n",
    "# Fill missing target_open with the next day's open price\n",
    "merged['target_open'] = merged['target_open'].fillna(merged['open'].shift(-1))\n",
    "\n",
    "print(\"Filled missing target_open values using next day's open price.\")\n",
    "print(f\"Remaining null target_open: {merged['target_open'].isna().sum()} (should be 1 for the last date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a47f406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2026-01-05 00:00:00+0000', tz='Etc/UTC')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7220145",
   "metadata": {},
   "source": [
    "## Prepare Data for Inference\n",
    "Remove rows with missing target_open (like today's date) and prepare features for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588b6788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2016-02-19 00:00:00+00:00 to 2026-01-05 00:00:00+00:00\n",
      "\n",
      "First few rows:\n",
      "                         date  sentiment_polarity  sentiment_neg  \\\n",
      "230 2016-02-19 00:00:00+00:00               0.994          0.023   \n",
      "852 2017-10-05 00:00:00+00:00               0.997          0.008   \n",
      "731 2017-11-27 00:00:00+00:00               0.997          0.008   \n",
      "845 2017-11-30 00:00:00+00:00               0.989          0.021   \n",
      "14  2018-01-31 00:00:00+00:00               0.995          0.009   \n",
      "\n",
      "     sentiment_neu  sentiment_pos       open  target_open  \n",
      "230          0.869          0.108  21.762466    21.832740  \n",
      "852          0.925          0.067  35.978361    36.162716  \n",
      "731          0.926          0.065  40.995288    40.819656  \n",
      "845          0.804          0.174  39.913317    39.800894  \n",
      "14           0.937          0.054  39.079592    39.149856  \n",
      "\n",
      "Last few rows:\n",
      "                          date  sentiment_polarity  sentiment_neg  \\\n",
      "0    2025-12-23 00:00:00+00:00            0.298535       0.131626   \n",
      "1134 2025-12-30 00:00:00+00:00            0.118052       0.183541   \n",
      "1135 2025-12-31 00:00:00+00:00            0.238841       0.181459   \n",
      "1136 2026-01-02 00:00:00+00:00            0.069390       0.317900   \n",
      "1137 2026-01-05 00:00:00+00:00            0.262844       0.159274   \n",
      "\n",
      "      sentiment_neu  sentiment_pos        open  target_open  \n",
      "0          0.438214       0.430160  270.839996   272.339996  \n",
      "1134       0.514867       0.301592  272.809998   273.059998  \n",
      "1135       0.398241       0.420300  273.059998   272.049988  \n",
      "1136       0.294810       0.387290  272.049988   270.714996  \n",
      "1137       0.418608       0.422118  270.714996          NaN  \n"
     ]
    }
   ],
   "source": [
    "# Sort by date to see the data range\n",
    "merged_sorted = merged.sort_values('date')\n",
    "print(f\"Date range: {merged_sorted['date'].min()} to {merged_sorted['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(merged_sorted.head())\n",
    "print(f\"\\nLast few rows:\")\n",
    "print(merged_sorted.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102ee710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data: 1137 rows with valid target_open\n",
      "\n",
      "Null target_open count: 1\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with null target_open (typically today's date)\n",
    "inference_data = merged[merged['target_open'].notna()].copy()\n",
    "print(f\"Filtered data: {len(inference_data)} rows with valid target_open\")\n",
    "print(f\"\\nNull target_open count: {merged['target_open'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5d428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1137, 5)\n",
      "Features: ['sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'opening_prices_open']\n",
      "\n",
      "Column names in inference_data: ['date', 'sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'opening_prices_open', 'target_open']\n"
     ]
    }
   ],
   "source": [
    "# Prepare features (X) and target (y) for inference\n",
    "# Features: sentiment columns and open price\n",
    "# Note: Model was trained with 'opening_prices_open' not 'open', so we need to rename\n",
    "feature_cols = ['sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'opening_prices_open']\n",
    "\n",
    "# Rename the open column to match training feature names\n",
    "inference_data = inference_data.rename(columns={'open': 'opening_prices_open'})\n",
    "\n",
    "X_inference = inference_data[feature_cols]\n",
    "y_actual = inference_data['target_open']\n",
    "\n",
    "print(f\"Features shape: {X_inference.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"\\nColumn names in inference_data: {inference_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5e5cc",
   "metadata": {},
   "source": [
    "## Load Model from Hopsworks Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8909a380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5e27a792ad46659506807f8e25f716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/463798 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: /var/folders/2x/6wpkl49n4bsfkkvhr3gm3tt00000gn/T/b3857a55-2175-48ff-b8ac-e7e027bebc70/sentiment_stock_price_model_AAPL/2\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "# Get model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Retrieve the model\n",
    "model_name = \"sentiment_stock_price_model_AAPL\"\n",
    "model = mr.get_model(model_name, version=2)\n",
    "\n",
    "# Download model artifacts to local directory\n",
    "model_dir = model.download()\n",
    "print(f\"Model downloaded to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "763c5306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /var/folders/2x/6wpkl49n4bsfkkvhr3gm3tt00000gn/T/b3857a55-2175-48ff-b8ac-e7e027bebc70/sentiment_stock_price_model_AAPL/2/model.json\n"
     ]
    }
   ],
   "source": [
    "# Load the XGBoost model\n",
    "model_path = os.path.join(model_dir, \"model.json\")\n",
    "xgb_model = xgboost.XGBRegressor()\n",
    "xgb_model.load_model(model_path)\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a084a",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a87ad29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions made for 1137 dates\n",
      "\n",
      "Last 10 predictions:\n",
      "                          date        open  target_open  predicted_open\n",
      "985  2024-11-19 00:00:00+00:00  225.958047   227.033178      222.754395\n",
      "448  2024-11-20 00:00:00+00:00  227.033178   227.849499      231.013245\n",
      "336  2024-11-22 00:00:00+00:00  227.033189   230.417878      229.423584\n",
      "1118 2024-11-25 00:00:00+00:00  230.417878   232.279472      231.898621\n",
      "407  2024-11-26 00:00:00+00:00  232.279472   233.414318      231.898621\n",
      "1088 2024-11-27 00:00:00+00:00  233.414318   233.752786      231.898621\n",
      "0    2025-12-23 00:00:00+00:00  270.839996   272.339996      224.523178\n",
      "1134 2025-12-30 00:00:00+00:00  272.809998   273.059998      224.150162\n",
      "1135 2025-12-31 00:00:00+00:00  273.059998   272.049988      223.421127\n",
      "1136 2026-01-02 00:00:00+00:00  272.049988   270.714996      224.267426\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_inference)\n",
    "\n",
    "# Create a results dataframe (use the renamed column)\n",
    "results = inference_data[['date', 'opening_prices_open', 'target_open']].copy()\n",
    "results['predicted_open'] = y_pred\n",
    "\n",
    "# Rename back for display clarity\n",
    "results = results.rename(columns={'opening_prices_open': 'open'})\n",
    "\n",
    "print(f\"Predictions made for {len(results)} dates\")\n",
    "print(\"\\nLast 10 predictions:\")\n",
    "print(results.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd3d40",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14df6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 18:23:03,520 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2026-01-05 18:23:03,520 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2026-01-05 18:23:03,521 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2026-01-05 18:23:03,522 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2026-01-05 18:23:03,522 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2026-01-05 18:23:03,523 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "==================================================\n",
      "INFERENCE PERFORMANCE METRICS\n",
      "==================================================\n",
      "Mean Squared Error (MSE):  8.9943\n",
      "Root Mean Squared Error (RMSE): 2.9991\n",
      "Mean Absolute Error (MAE): 0.5617\n",
      "R² Score: 0.9944\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_actual, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_actual, y_pred)\n",
    "r2 = r2_score(y_actual, y_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"INFERENCE PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Squared Error (MSE):  {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7186472c",
   "metadata": {},
   "source": [
    "## Recent Predictions (Including Unvalidated)\n",
    "Show predictions for the most recent dates, including today's prediction for tomorrow (which can't be validated yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a65bc246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 recent dates without validation data (target_open is null)\n",
      "These are dates: [Timestamp('2026-01-05 00:00:00+0000', tz='Etc/UTC')]\n",
      "\n",
      "============================================================\n",
      "PREDICTIONS FOR RECENT DATES (Not Yet Validated)\n",
      "============================================================\n",
      "                     date  current_open  predicted_next_day_open\n",
      "2026-01-05 00:00:00+00:00    270.714996               224.588852\n",
      "============================================================\n",
      "\n",
      "Note: These predictions are for the next trading day's opening price.\n",
      "They cannot be validated yet because that trading day hasn't occurred.\n"
     ]
    }
   ],
   "source": [
    "# Check if there are rows with null target_open (recent dates we can predict but not validate)\n",
    "unvalidated_data = merged[merged['target_open'].isna()].copy()\n",
    "\n",
    "if len(unvalidated_data) > 0:\n",
    "    print(f\"Found {len(unvalidated_data)} recent dates without validation data (target_open is null)\")\n",
    "    print(f\"These are dates: {unvalidated_data['date'].tolist()}\")\n",
    "    \n",
    "    # Rename column to match model expectations\n",
    "    unvalidated_data = unvalidated_data.rename(columns={'open': 'opening_prices_open'})\n",
    "    \n",
    "    # Make predictions for these dates\n",
    "    X_recent = unvalidated_data[feature_cols]\n",
    "    y_pred_recent = xgb_model.predict(X_recent)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    recent_results = unvalidated_data[['date', 'opening_prices_open']].copy()\n",
    "    recent_results['predicted_next_day_open'] = y_pred_recent\n",
    "    recent_results = recent_results.rename(columns={'opening_prices_open': 'current_open'})\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PREDICTIONS FOR RECENT DATES (Not Yet Validated)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(recent_results.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNote: These predictions are for the next trading day's opening price.\")\n",
    "    print(\"They cannot be validated yet because that trading day hasn't occurred.\")\n",
    "else:\n",
    "    print(\"No recent unvalidated dates found. All data has known target_open values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46ae7dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete Prediction Timeline:\n",
      "Validated predictions (with actual outcomes): 1137 dates\n",
      "  Date range: 2016-02-19 00:00:00+00:00 to 2026-01-02 00:00:00+00:00\n",
      "\n",
      "Unvalidated predictions (future): 1 dates\n",
      "  Date range: 2026-01-05 00:00:00+00:00 to 2026-01-05 00:00:00+00:00\n",
      "\n",
      "  Latest prediction: On 2026-01-05, \n",
      "  predict next trading day open = $224.59\n"
     ]
    }
   ],
   "source": [
    "# Show a complete timeline: validated predictions + recent unvalidated predictions\n",
    "print(\"\\nComplete Prediction Timeline:\")\n",
    "print(f\"Validated predictions (with actual outcomes): {len(results)} dates\")\n",
    "print(f\"  Date range: {results['date'].min()} to {results['date'].max()}\")\n",
    "\n",
    "if len(unvalidated_data) > 0:\n",
    "    print(f\"\\nUnvalidated predictions (future): {len(recent_results)} dates\")\n",
    "    print(f\"  Date range: {recent_results['date'].min()} to {recent_results['date'].max()}\")\n",
    "    print(f\"\\n  Latest prediction: On {recent_results['date'].max().date()}, \")\n",
    "    print(f\"  predict next trading day open = ${recent_results['predicted_next_day_open'].iloc[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae6bb2",
   "metadata": {},
   "source": [
    "## Store Predictions\n",
    "Save the predictions to a new Feature Group `opening_price_preds` for use in dashboards or external applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99f2c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to store 1 new predictions...\n",
      "                       date  current_open  predicted_open\n",
      "0 2026-01-05 00:00:00+00:00    270.714996      224.588852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 1/1 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: opening_price_preds_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1267871/jobs/named/opening_price_preds_1_offline_fg_materialization/executions\n",
      "2026-01-05 18:23:21,246 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2026-01-05 18:23:27,752 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-05 18:25:11,491 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2026-01-05 18:25:11,677 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-05 18:25:23,877 INFO: Execution finished successfully.\n",
      "Successfully inserted predictions into 'opening_price_preds' feature group\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Store Predictions in Feature Store\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Initialize an empty DataFrame for all predictions\n",
    "preds_to_insert = pd.DataFrame()\n",
    "\n",
    "# We only need to insert new predictions (recent/unvalidated ones)\n",
    "# The historical validated predictions are likely already in the Feature Store from previous runs.\n",
    "if 'recent_results' in locals() and not recent_results.empty:\n",
    "    recent_preds = recent_results[['date', 'current_open', 'predicted_next_day_open']].copy()\n",
    "    recent_preds = recent_preds.rename(columns={'predicted_next_day_open': 'predicted_open'})\n",
    "    preds_to_insert = pd.concat([preds_to_insert, recent_preds])\n",
    "\n",
    "# 3. Insert into Feature Store\n",
    "if not preds_to_insert.empty:\n",
    "    # Sort by date\n",
    "    preds_to_insert = preds_to_insert.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Preparing to store {len(preds_to_insert)} new predictions...\")\n",
    "    print(preds_to_insert)\n",
    "\n",
    "    # Get or create the feature group\n",
    "    preds_fg = fs.get_or_create_feature_group(\n",
    "        name=\"opening_price_preds\",\n",
    "        version=1,\n",
    "        description=\"Predictions for AAPL opening prices (Next Day Open)\",\n",
    "        primary_key=[\"date\"],\n",
    "        event_time=\"date\"\n",
    "    )\n",
    "    \n",
    "    # Insert data\n",
    "    preds_fg.insert(preds_to_insert, wait=True)\n",
    "    print(\"Successfully inserted predictions into 'opening_price_preds' feature group\")\n",
    "else:\n",
    "    print(\"No new predictions found to store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c477a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 18:25:23,914 WARNING: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "Successfully exported 1138 records to ../docs/predictions.json\n",
      "Updated JSON structure to include 'current_open' (today's price) and 'future_open' (target).\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Export Data for GitHub Pages Dashboard\n",
    "# ---------------------------------------------------------\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare Validated Data (Past)\n",
    "# Merge results with sentiment from inference_data\n",
    "# results columns: date, opening_prices_open (renamed to 'open' then 'current_open' in previous cells), target_open, predicted_open\n",
    "# Let's re-fetch from 'results' dataframe which should have 'date', 'open', 'target_open', 'predicted_open'\n",
    "# Note: In cell 26, 'results' was created with: results = inference_data[['date', 'opening_prices_open', 'target_open']].copy()\n",
    "# and then renamed 'opening_prices_open' -> 'open'.\n",
    "\n",
    "web_data_past = results.merge(inference_data[['date', 'sentiment_polarity']], on='date', how='left')\n",
    "web_data_past = web_data_past.rename(columns={\n",
    "    'open': 'current_open',       # Price on 'date'\n",
    "    'target_open': 'future_open', # Price on 'date'+1 (Target)\n",
    "    'sentiment_polarity': 'sentiment'\n",
    "})\n",
    "web_data_past = web_data_past[['date', 'current_open', 'future_open', 'predicted_open', 'sentiment']]\n",
    "\n",
    "# 2. Prepare Recent Data (Future/Unvalidated)\n",
    "if 'recent_results' in locals() and not recent_results.empty:\n",
    "    # recent_results columns: date, current_open, predicted_next_day_open\n",
    "    web_data_future = recent_results.merge(unvalidated_data[['date', 'sentiment_polarity']], on='date', how='left')\n",
    "    web_data_future = web_data_future.rename(columns={\n",
    "        'predicted_next_day_open': 'predicted_open',\n",
    "        'sentiment_polarity': 'sentiment'\n",
    "    })\n",
    "    web_data_future['future_open'] = None # We don't know the future yet\n",
    "    \n",
    "    web_data_future = web_data_future[['date', 'current_open', 'future_open', 'predicted_open', 'sentiment']]\n",
    "    \n",
    "    # Combine\n",
    "    web_df = pd.concat([web_data_past, web_data_future])\n",
    "else:\n",
    "    web_df = web_data_past\n",
    "\n",
    "# 3. Format Date and Sort\n",
    "web_df['date'] = web_df['date'].dt.strftime('%Y-%m-%d')\n",
    "web_df = web_df.sort_values('date')\n",
    "\n",
    "# 4. Save to JSON\n",
    "output_path = '../docs/predictions.json'\n",
    "os.makedirs('../docs', exist_ok=True)\n",
    "\n",
    "# Replace NaN with None (which becomes null in JSON)\n",
    "web_df = web_df.replace(np.nan, None)\n",
    "\n",
    "json_data = web_df.to_dict(orient='records')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "print(f\"Successfully exported {len(json_data)} records to {output_path}\")\n",
    "print(\"Updated JSON structure to include 'current_open' (today's price) and 'future_open' (target).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
