{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbff96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c703a7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:57:27,676 INFO: Initializing external client\n",
      "2025-12-23 19:57:27,677 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-12-23 19:57:29,651 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1267871\n",
      "Successfully connected to Feature Store: a1id2223_featurestore\n",
      "HSFS Version: 4.2.10\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "# 1. Login\n",
    "project = hopsworks.login()\n",
    "\n",
    "# 2. Get the Feature Store (This triggers the metadata check)\n",
    "try:\n",
    "    fs = project.get_feature_store(os.getenv(\"FEATURE_STORE_NAME\"))\n",
    "    print(f\"Successfully connected to Feature Store: {fs.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Feature Store Connection Error: {e}\")\n",
    "\n",
    "# 3. Check versions\n",
    "print(f\"HSFS Version: {hsfs.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eebc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/sambarati/Documents/GitHub/nlp-stock-prediction/data/apple_news_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e59c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment data: 1574 rows, 1574 unique dates\n",
      "sentiment_polarity    0.994\n",
      "sentiment_neg         0.023\n",
      "sentiment_neu         0.869\n",
      "sentiment_pos         0.108\n",
      "Name: 2016-02-19 00:00:00+00:00, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Aggregate sentiment data by date\n",
    "# Convert date column to datetime first\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "# Normalize to date only (remove time component)\n",
    "data['date'] = data['date'].dt.normalize()\n",
    "\n",
    "# Since we may have multiple news articles per day, calculate daily averages\n",
    "sentiment_daily = data.groupby('date').agg({\n",
    "    'sentiment_polarity': 'mean',\n",
    "    'sentiment_neg': 'mean',\n",
    "    'sentiment_neu': 'mean',\n",
    "    'sentiment_pos': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Get smallest and highest dates\n",
    "from_date = sentiment_daily['date'].min()\n",
    "to_date = sentiment_daily['date'].max()\n",
    "# Set index to date for later join\n",
    "sentiment_daily.set_index('date', inplace=True)\n",
    "\n",
    "print(f\"Sentiment data: {len(sentiment_daily)} rows, {sentiment_daily.index.nunique()} unique dates\")\n",
    "print(sentiment_daily.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94019253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2016-02-19 00:00:00-05:00', '2016-02-22 00:00:00-05:00',\n",
      "               '2016-02-23 00:00:00-05:00', '2016-02-24 00:00:00-05:00',\n",
      "               '2016-02-25 00:00:00-05:00', '2016-02-26 00:00:00-05:00',\n",
      "               '2016-02-29 00:00:00-05:00', '2016-03-01 00:00:00-05:00',\n",
      "               '2016-03-02 00:00:00-05:00', '2016-03-03 00:00:00-05:00',\n",
      "               ...\n",
      "               '2024-11-13 00:00:00-05:00', '2024-11-14 00:00:00-05:00',\n",
      "               '2024-11-15 00:00:00-05:00', '2024-11-18 00:00:00-05:00',\n",
      "               '2024-11-19 00:00:00-05:00', '2024-11-20 00:00:00-05:00',\n",
      "               '2024-11-21 00:00:00-05:00', '2024-11-22 00:00:00-05:00',\n",
      "               '2024-11-25 00:00:00-05:00', '2024-11-26 00:00:00-05:00'],\n",
      "              dtype='datetime64[ns, America/New_York]', name='Date', length=2209, freq=None)\n"
     ]
    }
   ],
   "source": [
    "stock_data = yf.Ticker(\"AAPL\") \n",
    "stock_data_price_history = stock_data.history(period=\"10y\") # Get 10 yr price history\n",
    "stock_data_clean = stock_data_price_history[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "stock_data_clean = stock_data_clean[from_date:to_date]\n",
    "print(stock_data_clean.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddccecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2016-02-19 05:00:00', '2016-02-22 05:00:00',\n",
      "               '2016-02-23 05:00:00', '2016-02-24 05:00:00',\n",
      "               '2016-02-25 05:00:00', '2016-02-26 05:00:00',\n",
      "               '2016-02-29 05:00:00', '2016-03-01 05:00:00',\n",
      "               '2016-03-02 05:00:00', '2016-03-03 05:00:00',\n",
      "               ...\n",
      "               '2024-11-13 05:00:00', '2024-11-14 05:00:00',\n",
      "               '2024-11-15 05:00:00', '2024-11-18 05:00:00',\n",
      "               '2024-11-19 05:00:00', '2024-11-20 05:00:00',\n",
      "               '2024-11-21 05:00:00', '2024-11-22 05:00:00',\n",
      "               '2024-11-25 05:00:00', '2024-11-26 05:00:00'],\n",
      "              dtype='datetime64[ns]', name='Date', length=2209, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Remove timezone from stock data to match sentiment data (which is timezone-naive)\n",
    "stock_data_clean.index = stock_data_clean.index.tz_convert(None)\n",
    "print(stock_data_clean.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cc91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock tz: None, Sentiment tz: None\n",
      "Sentiment index sample: DatetimeIndex(['2016-02-19', '2017-10-05', '2017-11-27', '2017-11-30',\n",
      "               '2018-01-31'],\n",
      "              dtype='datetime64[ns]', name='date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Remove timezone from sentiment data to match stock data (which is timezone-naive)\n",
    "sentiment_daily.index = sentiment_daily.index.tz_localize(None)\n",
    "print(f\"Stock tz: {stock_data_clean.index.tz}, Sentiment tz: {sentiment_daily.index.tz}\")\n",
    "print(f\"Sentiment index sample: {sentiment_daily.index[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c6888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock dates: DatetimeIndex(['2016-02-19', '2016-02-22', '2016-02-23', '2016-02-24',\n",
      "               '2016-02-25'],\n",
      "              dtype='datetime64[ns]', name='date', freq=None)\n",
      "Sentiment dates: DatetimeIndex(['2016-02-19', '2017-10-05', '2017-11-27', '2017-11-30',\n",
      "               '2018-01-31'],\n",
      "              dtype='datetime64[ns]', name='date', freq=None)\n",
      "\n",
      "Stock data has 0 duplicate dates\n",
      "Sentiment data has 0 duplicate dates\n"
     ]
    }
   ],
   "source": [
    "# Normalize stock data index to date only (remove time component)\n",
    "stock_data_clean.index = stock_data_clean.index.normalize()\n",
    "# Sentiment data is already normalized\n",
    "\n",
    "# Rename both indices to 'date' (lowercase) for consistency\n",
    "stock_data_clean.index.name = 'date'\n",
    "sentiment_daily.index.name = 'date'\n",
    "\n",
    "print(f\"Stock dates: {stock_data_clean.index[:5]}\")\n",
    "print(f\"Sentiment dates: {sentiment_daily.index[:5]}\")\n",
    "print(f\"\\nStock data has {stock_data_clean.index.duplicated().sum()} duplicate dates\")\n",
    "print(f\"Sentiment data has {sentiment_daily.index.duplicated().sum()} duplicate dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66824c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                  2016-02-19 00:00:00\n",
      "open                             21.76247\n",
      "high                            21.934757\n",
      "low                             21.717132\n",
      "close                           21.771538\n",
      "volume                          141496800\n",
      "sentiment_polarity                  0.994\n",
      "sentiment_neg                       0.023\n",
      "sentiment_neu                       0.869\n",
      "sentiment_pos                       0.108\n",
      "Name: 0, dtype: object\n",
      "Merged data shape: (1132, 10)\n",
      "Columns: ['date', 'open', 'high', 'low', 'close', 'volume', 'sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos']\n"
     ]
    }
   ],
   "source": [
    "# Merge on the date index\n",
    "merged_data = stock_data_clean.join(sentiment_daily, how='inner')\n",
    "\n",
    "# Reset index to convert date from index to column\n",
    "merged_data = merged_data.reset_index()\n",
    "\n",
    "# Rename columns to lowercase to match feature group schema\n",
    "merged_data.columns = merged_data.columns.str.lower()\n",
    "\n",
    "print(merged_data.iloc[0])\n",
    "\n",
    "\n",
    "# Display the merged datamerged_data.size\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"Columns: {merged_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d880362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature group for merged sentiment and stock price data\n",
    "fg = fs.get_or_create_feature_group(\n",
    "    name=\"stock_price_and_history\", \n",
    "    description=\"joined stock price and sentiment history\",\n",
    "    version = 1,\n",
    "    primary_key = ['date'],\n",
    "    event_time = 'date'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1863188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1267871/fs/1262659/fg/1878362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 1132/1132 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: stock_price_and_history_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1267871/jobs/named/stock_price_and_history_1_offline_fg_materialization/executions\n",
      "2025-12-22 14:34:19,184 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2025-12-22 14:34:22,364 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-12-22 14:36:16,965 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-12-22 14:36:17,135 INFO: Waiting for log aggregation to finish.\n",
      "2025-12-22 14:36:42,601 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('stock_price_and_history_1_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg.insert(merged_data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec268251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos']\n",
      "Duplicate columns: []\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24982 entries, 0 to 24981\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   Date                24982 non-null  datetime64[ns]\n",
      " 1   Open                24982 non-null  float64       \n",
      " 2   High                24982 non-null  float64       \n",
      " 3   Low                 24982 non-null  float64       \n",
      " 4   Close               24982 non-null  float64       \n",
      " 5   Volume              24982 non-null  int64         \n",
      " 6   sentiment_polarity  24971 non-null  float64       \n",
      " 7   sentiment_neg       24971 non-null  float64       \n",
      " 8   sentiment_neu       24971 non-null  float64       \n",
      " 9   sentiment_pos       24971 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(8), int64(1)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate columns\n",
    "print(\"Columns:\", merged_data.columns.tolist())\n",
    "print(\"Duplicate columns:\", merged_data.columns[merged_data.columns.duplicated()].tolist())\n",
    "print(\"\\nDataFrame info:\")\n",
    "merged_data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
